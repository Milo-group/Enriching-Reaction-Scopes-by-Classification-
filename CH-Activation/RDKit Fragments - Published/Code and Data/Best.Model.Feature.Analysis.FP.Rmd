---
title: "CH Activation - Best Model Feature Analysis"
output:
  word_document:
    highlight: null
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE,
                      comment = NA,
                      message = FALSE,
                      warning = FALSE,
                      results = 'asis',
                      error = TRUE,
                      fig.height = 8)

# The following packages are installed and loaded 
library(data.table)
library(tidyr)
library(ggplot2)
library(ggrepel)
library(scales)
library(reshape2)
library(tibble)
library(caret)
library(plyr)
library(dplyr)
library(default)

default(data.frame) <- list(check.names = FALSE)
```

```{r Functions, echo=TRUE, include=FALSE}
# k.fold.multinom description:
# 
# Takes a formula, dataset, number of folds and the output vector. 
# Uses nnet::multinom (see nnet v7.3-14 documentation - multinom).
# Divides data into folds, each time using a single fold as the test set
# and the rest as training. 
# Creates a predicted probalities and classes dataframe for each run 
# which is then combined to form a complete table.
# Returns a classifictaion table for predicted outcome vs. experimental observations,
# an accuracy measure based on that table and the probabilities table. 
stratified <- function(df, group, size) {
  df.interaction <- interaction(df[group], drop = TRUE)
  df.table <- table(df.interaction)
  df.split <- split(df, df.interaction)
  if (length(size) > 1) {
    if (length(size) != length(df.split))
      stop("Number of groups is ", length(df.split),
           " but number of sizes supplied is ", length(size))
    if (is.null(names(size))) {
      n <- setNames(size, names(df.split))
    } else {
      ifelse(all(names(size) %in% names(df.split)),
             n <- size[names(df.split)],
             stop("Named vector supplied with names ",
                  paste(names(size), collapse = ", "),
                  "\n but the names for the group levels are ",
                  paste(names(df.split), collapse = ", ")))
    }
  } else if (size < 1) {
    n <- round(df.table * size, digits = 0)
  } else if (size >= 1) {
    if (all(df.table >= size) || isTRUE(replace)) {
      n <- setNames(rep(size, length.out = length(df.split)),
                    names(df.split))
    } else {
      message(
        "Some groups\n---",
        paste(names(df.table[df.table < size]), collapse = ", "),
        "---\ncontain fewer observations",
        " than desired number of samples.\n",
        "All observations have been returned from those groups.")
      n <- c(sapply(df.table[df.table >= size], function(x) x = size),
             df.table[df.table < size])
    }
  }
  temp <- lapply(
    names(df.split),
    function(x) df.split[[x]][sample(df.table[x],
                                     n[x], replace = F), ])
  set1 <- do.call("rbind", temp)
  set1
}

k.fold.multinom <- function(formula, data, folds = NULL,
                            outcome.column,
                            stratify = F,
                            sample.vector = floor(summary(data$class)/min(summary(data$class)))) {
  models <- list()
  probalities <- list()
  acc <- list()
  class.pred <- list()
  if (stratify == T && !is.null(sample.vector) && is.null(folds)) {
    sets <- list()
    for (i in 1:ceiling(dim(data)[1] / sum(sample.vector))) {
      if (length(sets) == 0) {
        pool.data <- data
      } else {
        pool.data <- dplyr::setdiff(data, do.call(rbind, sets))
      }
      if (dim(pool.data)[1] < sum(sample.vector) & dim(pool.data)[1] > 0) {
        sets[[i]] <- pool.data
        pool.data <- data.frame()
      }
      if (sum(dim(pool.data)) > 0 & any(summary(pool.data$class) == 0)) {
        for (i in 0:nrow(pool.data)) {
          if (i > 0) sets[[i]] <- rbind(sets[[i]], pool.data[i, ])
        }
        is_empty <- function(x) (nrow(x) == 0 || is.null(x))
        sets <- sets[sapply(sets, is_empty) == FALSE]
      } else {
        if (sum(dim(pool.data)) > 0) suppressWarnings(sets[[i]] <- stratified(pool.data, 'class', sample.vector))
      }
      if (i == ceiling(dim(data)[1] / sum(sample.vector))) {
        check <- dim(do.call(rbind, sets)) == dim(data)
        if (!all(check)) {
          print("Wasn't able to stritify as wanted. Check with another sample vector.")
        }
      }
    }
    for (i in 1:length(sets)) {
      sets[[i]] <- dplyr::mutate(sets[[i]], split.assign = i)
    }
    new_dat <- do.call(rbind, sets)
    new_dat <- dplyr::arrange(new_dat, flag)
    for (i in 1:length(sort(unique(new_dat$split.assign)))) {
      train <- new_dat[new_dat$split.assign != i,]
      test <- new_dat[new_dat$split.assign == i,]
      models[[match(i,sort(unique(new_dat$split.assign)))]] <- nnet::multinom(formula,
                                                                              data = train,
                                                                              maxit = 2000,
                                                                              trace = FALSE)
      probalities[[match(i,sort(unique(new_dat$split.assign)))]] <- data.frame(predict(models[[match(i,sort(unique(new_dat$split.assign)))]], 
                                                                                       newdata = test,
                                                                                       "probs")*100)
      class.pred[[match(i,sort(unique(new_dat$split.assign)))]] <- data.frame(predict(models[[match(i,sort(unique(new_dat$split.assign)))]],
                                                                                      newdata = test,
                                                                                      "class"))
      probalities[[match(i,sort(unique(new_dat$split.assign)))]] <- cbind(probalities[[match(i,sort(unique(new_dat$split.assign)))]],
                                                                          class.pred[[match(i,sort(unique(new_dat$split.assign)))]],
                                                                          test$flag)
      names(probalities[[match(i,sort(unique(new_dat$split.assign)))]]) <- c('A','B','C','D','prediction','flag')
    }
  } else {
    if (folds == nrow(data)) {
      split.assign <- sample(1:folds, nrow(data), replace = F)
    } else {
      split.assign <- caret::createFolds(1:dim(data)[1], folds, list = F)
    }
    new_dat <- cbind(data, split.assign)
    for (i in 1:folds) {
      train <- new_dat[split.assign != i,]
      test <- new_dat[split.assign == i,]
      models[[match(i,1:folds)]] <- nnet::multinom(formula,
                                                   data = train,
                                                   maxit = 2000,
                                                   trace = FALSE)
      if (folds == nrow(data)) {
        probalities[[match(i,1:folds)]] <- data.table::transpose(data.frame(predict(models[[match(i,1:folds)]], 
                                                                                    newdata = test, "probs")*100))
      } else {
        probalities[[match(i,1:folds)]] <- data.frame(predict(models[[match(i,1:folds)]], 
                                                              newdata = test, "probs")*100)
      }
      class.pred[[match(i,1:folds)]] <- data.frame(predict(models[[match(i,1:folds)]], newdata = test, "class"))
      probalities[[match(i,1:folds)]] <- cbind(probalities[[match(i,1:folds)]],class.pred[[match(i,1:folds)]],test$flag)
      names(probalities[[match(i,1:folds)]]) <- c('A','B','C','D','prediction','flag')
    }
  }
  
  probs <- data.frame(do.call(rbind, probalities))
  probs <- probs[order(probs$flag),]
  probs[,1:4] <- round(probs[,1:4],digits = 0)
  pred <- probs[,5]
  pred <- plyr::mapvalues(pred, 
                          from=c("1","2","3","4"), 
                          to=c("A","B","C","D"))
  actual <- data[[outcome.column]]
  actual <- plyr::mapvalues(actual, 
                            from=c("1","2","3","4"), 
                            to=c("A","B","C","D"))
  ct <- table(actual, pred)
  acc <- round((sum(diag(ct))/sum(ct))*100,2)
  ct.df <- data.frame(ct)
  TP <- ct.df$Freq[ct.df$actual == ct.df$pred]
  TP_FN <- list()
  TN <- list()
  TN_FP <- list()
  J <- list()
  for (i in levels(ct.df$actual)) {
    TP_FN[[i]] <- ct.df$Freq[ct.df$actual == i]
    TP_FN[[i]] <- sum(TP_FN[[i]])
    TP_FN[[i]] <- TP[which(levels(ct.df$actual) == i)]/TP_FN[[i]]
    TN[[i]] <- sum(ct.df$Freq[dplyr::intersect(which(ct.df$pred != i),
                                               which(ct.df$actual != i))])
    TN_FP[[i]] <- sum(ct.df$Freq[(ct.df$actual != i)])
    TN_FP[[i]] <- TN[[i]]/TN_FP[[i]]
    J[[i]] <- TP_FN[[i]] + TN_FP[[i]] - 1
  }
  J.small <- J[which.min(summary(data$class))]
  return(list(acc, J.small, ct, probs))
}

# kf.iter description:
# 
# Takes a formula, dataset, number of folds and the output vector along with 
# number of iterations. 
# Runs k.fold.multinom by iterations numbers and returns the average 
# accuracy over all iterations, the best and worst accuracy and 
# best and worst classification tables.

kf.iter <- function(formula, data, folds = NULL, out.col=which(colnames(data) == 'class'),
                    stratify = F,
                    sample.vector = floor(summary(data$class)/min(summary(data$class))),
                    iterations, verbose = F) {
  iter.list <- list()
  ct.list <- list()
  for (i in 1:iterations) {
    mod <- k.fold.multinom(formula, data, folds, out.col, stratify, sample.vector)
    iter.list[[match(i,1:iterations)]] <- mod[[1]]
    ct.list[[match(i,1:iterations)]] <- mod[[3]]
  }
  over.all.accuracy <- round(Reduce(`+`,iter.list)/iterations,digits = 2)
  best <- iter.list[which.max(iter.list)]
  worst <- iter.list[which.min(iter.list)]
  Accuracies.tab <- cbind(over.all.accuracy, best, worst)
  colnames(Accuracies.tab) <- c("Avergae Accuracy", "Best", "Worst")
  Accuracies <- knitr::kable(Accuracies.tab,
                             caption = "\n\nCross Validation Accuracies - Avergare(left), Best (middle) and Worst (right)")
  tab <- suppressMessages(cbind(dcast(data.frame(ct.list[which.max(iter.list)]),actual~pred),
                                rep("***",4),
                                dcast(data.frame(ct.list[which.min(iter.list)]),actual~pred)))
  names(tab)[6] <- ''
  cts <- knitr::kable(tab,
                      caption = "\n\nCross Validation Results - Best (left) and Worst (True) Classification Tables")
    if (!is.null(folds)) {
    Headline.caption <- "\n\n Leave One Out Cross Validation"
  }
  if (stratify == T) {
   Headline.caption <- paste("\n\nStratified (# of samples in the smallest group) Folds Cross Validation - ",
                             iterations,
                             "Iterations")  
  dummy.df <- data.frame(NULL)
  Headline <- knitr::kable(dummy.df, caption = Headline.caption, )
  }
  if (verbose == T) { 
    print(cts)
    print(Accuracies)
  }
  invisible(over.all.accuracy)
}

### sub_model_log
sub_model_log_McFadden <- function(data, out.col = which(colnames(data) == 'class'),
                                   min = 4, max = floor(dim(data)[1]/5)) {
  output <- stringr::str_c('`',names(data[out.col]),'`')
  vars <- names(data[,-out.col])
  vars <- vars[vars != 'flag']
  for (i in 1:length(vars)) {
    vars[i] <- stringr::str_c('`',vars[i],'`')
  }
  comb.list <- list()
  R2.list <- list()
  for (i in min:max) {
    comb.list[[i]] <- data.frame(aperm(combn(vars,i)),stringsAsFactors = F)
    comb.list[[i]][,dim(comb.list[[i]])[2]+1] <- do.call(paste, 
                                                         c(comb.list[[i]][names(comb.list[[i]])],
                                                           sep=" + "))
    names(comb.list[[i]])[dim(comb.list[[i]])[2]] <- 'formula'
    for (co in names(comb.list[[i]])[1:length(names(comb.list[[i]]))-1]) comb.list[[i]][co] <- NULL
  }
  comb.list <- plyr::compact(comb.list)
  forms <- do.call(rbind,comb.list)
  names(forms) <- 'formula'
  for (i in 1:dim(forms)[1]) {
    forms$formula[i] <- stringr::str_c(output,' ~ ',forms$formula[i])
    test.1 <- nnet::multinom(forms$formula[i],
                             data = data,
                             maxit = 2000, trace = F)
    test.0 <- nnet::multinom(class ~ 1,data = data, maxit = 2000, trace=F)
    loglik.0 <- as.numeric(nnet:::logLik.multinom(test.0))
    loglik.1 <- as.numeric(nnet:::logLik.multinom(test.1))
    R2.list[i] <- round((1 - (loglik.1/loglik.0)),digits = 3)
  }
  forms[,2] <- do.call(rbind,R2.list)
  names(forms)[2] <- 'McFadden R2'
  out.models <- head(dplyr::arrange(forms,desc(forms[,2])),10)
  return(out.models)
}
# mod.info description:
# 
# Takes a nnet::multinom model object.
# using the full data set as both training and testing sets, model information is extracted.
# Creates a class.table object to be used in confusion matrix plotting,
# computes and returnes varibale importance as the sum of each coefficient's absolute value
# over all predictied classes (See caret v6.0-86 documentation - varImp),
# accuracy based on classification table, 
# coeffcient's exponent as another measure of importance 
# and McFadden's pseudo R squared for multinomial logistic regression (T. Domencich and D. L. McFadden, 1975).
mod.info <- function(model, data, verbose = T, save.acc = F) {
  pred <- predict(test,newdata = data, 'class')
  pred <- plyr::mapvalues(pred, 
                    from=c("1","2","3","4"), 
                    to=c("A","B","C","D"), warn_missing = F)
  actual <- data$class
  actual <- plyr::mapvalues(actual, 
                      from=c("1","2","3","4"), 
                      to=c("A","B","C","D"), warn_missing = F)
  example <- tibble(actual, pred)
  all_vec <- as.factor(LETTERS[1:4])
  example[] <- lapply(example, factor, levels = all_vec)
  
  class.table <<- table(example)
  if (save.acc == T) Acc.print <<- round((sum(diag(class.table))/sum(class.table))*100,2)
  Accuracy <- paste(round((sum(diag(class.table))/sum(class.table))*100,2),"%",sep = '')
  test.0 <- nnet::multinom(class ~ 1,data = data, maxit = 2000, trace=F)
  test.1 <- test
  
  loglik.0 <- as.numeric(nnet:::logLik.multinom(test.0))
  loglik.1 <- as.numeric(nnet:::logLik.multinom(test.1))
  
  McFadden_R2 <- round((1 - (loglik.1/loglik.0)),digits = 3)
  st <- cbind(Accuracy,McFadden_R2)
  names(st) <- c("Accuracy", "McFadden_R2")
  stats <- knitr::kable(st, 
                        caption = "\n\nFull Model Stats - Overall Accuracy and Pseudo-R2")
  ce <- knitr::kable(coef(test),caption = "\n\nModel Coefficients")
  if (verbose == T) {
    print(stats)

    print(ce)
  }
}


# ct.plot description:
# 
# Takes a class table and computes;
# * Recall - TP/(TP+FN) & complimentary percentages of wrong prdicitions in each cell.
# * Precision - TP/(TP+FP) for each prediction class.
# * Accuracy - all.TP/total.
# Classifies into Right and Wrong predictions and presents class size.

ct.plot <- function(class.table, plot.title, conformation) {
  ct <- as.matrix(class.table)
  total <- rep(NA, nrow(ct))
  ct <- cbind(ct,total)
  for (i in 1:dim(ct)[1]) {
    ct[i,5] <- sum(ct[i,1:4])
  }
  total <- rep(NA, ncol(ct))
  ct <- rbind(ct,total)
  for (i in 1:dim(ct)[2]) {
    ct[dim(ct)[1],i] <- sum(ct[1:(dim(ct)[1] - 1),i])
  }
  ct <- melt(ct)
  names(ct) <- c('Exp','Pred','Freq')
  ct$Exp <- as.character(ct$Exp)
  ct$Pred <- as.character(ct$Pred)
  
  ct <- dplyr::mutate(ct, Legend=rep(NA, nrow(ct)))
  ct <- dplyr::mutate(ct,Proportions = rep(NA, nrow(ct)))
  
  
  
  for (i in as.numeric(row.names(ct[ct$Exp!='total' & ct$Pred!='total', ]))) {
    if (ct$Exp[[i]]=='A') {
      ct$Proportions[[i]] <- ct$Freq[[i]]/sum(ct$Freq[ct$Exp=='A' & ct$Pred!='total'])
    }
    if (ct$Exp[[i]]=='B') {
      ct$Proportions[[i]] <- ct$Freq[[i]]/sum(ct$Freq[ct$Exp=='B' & ct$Pred!='total'])
    }
    if (ct$Exp[[i]]=='C') {
      ct$Proportions[[i]] <- ct$Freq[[i]]/sum(ct$Freq[ct$Exp=='C' & ct$Pred!='total'])
    }
    if (ct$Exp[[i]]=='D') {
      ct$Proportions[[i]] <- ct$Freq[[i]]/sum(ct$Freq[ct$Exp=='D' & ct$Pred!='total'])
    }
    if (ct$Exp[i] == ct$Pred[i]) {
      ct$Legend[i] <- 'True'
    } else {
      ct$Legend[i] <- 'False'
    }
  } 
  ct$Proportion <- round(ct$Proportion,digits = 3)*100
  
  for (i in as.numeric(row.names(ct[ct$Exp =='total' | ct$Pred =='total', ]))) {
    if (ct$Pred[i]=='total' & ct$Exp[i]!='total') {
      ct$Proportion[i] <- (ct$Freq[i]/ct$Freq[ct$Exp == 'total' & ct$Pred == 'total'])*100
      ct$Legend[i] <- 'Size'
    }
    if (ct$Pred[i]!='total' & ct$Exp[i]=='total') {
      ct$Proportion[[i]] <- (ct$Freq[ct$Exp == ct$Pred[i] & ct$Pred == ct$Pred[i]]/ct$Freq[i])*100
      ct$Legend[i] <- 'Precision'
    }
    if (ct$Pred[i]=='total' & ct$Exp[i]=='total') {
      ct$Proportion[[i]] <- (sum(diag(class.table))/ct$Freq[i])*100
      ct$Legend[i] <- 'Accuracy'
    }
  } 
  ct$Proportion <- round(ct$Proportion,digits = 1)
  
  ct <- mutate(ct,value=ct$Proportion)
  
  for (i in 1:nrow(ct)) {
    if (ct$Legend[i]!='True' & ct$Legend[i]!='False') {
      ct$Proportion[i] <- 45
    }
  }
  
  ct[is.na(ct)] <- 0
  
  base <- ggplot(data = ct, 
                 mapping = aes(x =ordered(Pred, levels =sort(unique(Pred))),
                               y=ordered(Exp, levels =rev(sort(unique(Exp)))),
                               fill = Legend,
                               alpha = Proportion))+
    geom_tile(color = 'black',size = 1.5)+
    coord_fixed()+
    geom_text(aes(label=paste(Freq,"\n",
                              '(',value,'%',')',sep = '')),
              size=4,vjust = .5, fontface  = "bold", alpha = 1)+
    scale_fill_manual(values = c(True = '#6CAE75', False = '#8A0526',
                                 Size = '#FCDEBE', Precision = '#DCAB6B',
                                 Accuracy = '#247BA0'))+
    theme(axis.line = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank(),
          axis.text.x = element_text(size = 9,face = 'bold'),
          axis.text.y = element_text(size = 9,face = 'bold'),
          axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.ticks = element_blank())
  
  true.recall <- data.frame(ct$Proportion[ct$Exp == ct$Pred & ct$Exp != 'total'])
  row.names(true.recall) <- unique(as.character(ct$Exp[ct$Exp != 'total']))
  names(true.recall) <- 'Class Recall'
  Low.Recall <<- true.recall
  base + labs(title = plot.title,
              subtitle = 'Confusion Matrix',
              caption = conformation)
}

# prob.heatmap description:
# 
# Takes a nnet::multinom model object.
# Represents the final probabilities table as a heat map with a color gradient based
# on probability percentage. The correctly predicted case names are colored green,
# wrong but second in percentage are colored orange and absolute wrong is colored red.
# The experimental outcome is presented in its own column, color coded for convenience.

prob.heatmap <- function(model, data, plot.title, conformation) {
  pred <- predict(model,newdata = data, 'class')
  r.w <- as.character(pred) == as.character(data$class)
  probs <- round(signif(predict(model,newdata = data, 'probs')*100, 4))
  verif <- data.frame(cbind(as.numeric(as.character(data$class)), pred, r.w, probs, 
                            rep(NA, nrow(probs))))
  row.names(verif) <- row.names(probs)
  
  for (i in 1:dim(verif)[1]) {
    if (verif$r.w[i] == 1) { 
      verif$V7[i] <- "#66a182" # green
    } else {
      if (verif[i,verif[i,1]+3] == min(verif[i,4:7])) {
        verif$V7[i] <- '#d1495b'
      } else {
        if (is.na(verif$V8[i])) {
          verif$V7[i] <-  'tan1'
        }
      }
    }
  }
  
  pro.df <- data.frame(probs)
  pro.df <- rownames_to_column(pro.df)
  pro.df[,1] <- factor(pro.df[,1],levels = pro.df[,1])
  pro.df[,6] <- as.numeric(as.character(data$class))
  colnames(pro.df) <- c('Name',"A","B","C","D",'Exp')
  row.names(pro.df) <- row.names(probs)
  
  long <- melt(pro.df,id.vars = 'Name')
  long[,3] <- round(long[,3],digits = 2)
  long <- mutate(long, exp_shape=rep(NA,nrow(long)))
  
  
  for (i in 1:nrow(long)) {
    if (long$variable[i] == 'Exp' & long$value[i] == 1) {
      long$exp_shape[i] <- "A"
    }
    if (long$variable[i] == 'Exp' & long$value[i] == 2) {
      long$exp_shape[i] <- 'B'
    }
    if (long$variable[i] == 'Exp' & long$value[i] == 3) {
      long$exp_shape[i] <- 'C'
    }
    if (long$variable[i] == 'Exp' & long$value[i] == 4) {
      long$exp_shape[i] <- 'D'
    }
  }
  shape_vec <- long[long$variable == 'Exp',4]
  col_vec <- shape_vec
  
  for (i in 1:length(col_vec)) {
    if (col_vec[i] == "A") {
      col_vec[i] <- 'darkorchid'
    }
    if (col_vec[i] == 'B') {
      col_vec[i] <- "darkslategray"
    }
    if (col_vec[i] == 'C') {
      col_vec[i] <- "darkgoldenrod4"
    }
    if (col_vec[i] == 'D') {
      col_vec[i] <- "steelblue"
    }
  }
  
  
  
  prob.heatmap <- ggplot(mapping = aes(x = variable,
                                       y =ordered(Name, 
                                                  levels =rev(factor(pro.df$Name, 
                                                                     levels = pro.df$Name)))))+
    geom_tile(data = long[long$variable != 'Exp',], 
              color = 'black',aes(fill = value))+
    coord_fixed(ratio = 0.2)+
    geom_text(data = long[long$variable != 'Exp',], 
              aes(label=value))+
    scale_fill_gradient(name = "% probability",
                        low = "#FFFFFF",
                        high = "dodgerblue3",
                        guide = guide_colorbar(frame.colour = "black", 
                                               ticks.colour = "black"))+
    theme(axis.text.x = element_text(size = 9, face = 'bold',),
          axis.text.y = element_text(size = 9, face = 'bold', 
                                     colour = rev(verif$V7)),
          axis.title.y = element_blank(),
          axis.title.x = element_blank(),
          axis.line = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank(),
          axis.ticks = element_blank())+
    scale_x_discrete(position = "top",limits = levels(long$variable))+
    geom_tile(data = long[long$variable == 'Exp',],
              alpha = 0,inherit.aes = F,
              aes(x=rev(variable),
                  y=ordered(Name, levels =rev(factor(pro.df$Name, 
                                                         levels = pro.df$Name)))))+
    geom_text(data = long[long$variable == 'Exp',], label = shape_vec,
              size = 2.4, color = col_vec, fontface = 'bold')
  
  prob.heatmap + labs(title = plot.title,
              subtitle = 'Probability Heatmap',
              caption = conformation)
}

# simi.sampler description:
#
# Takes data and class as input, yields a subset of the class based
# on a cosine similarity test of the class samples with the class mean vector. 
# This tests how "similar" each sample is to the class itself. 
# The sampler takes the smallest class as the size of subset and measures the 
# the distances of all samples in the class with "poles" stationed in equal spacing 
# from the most similar to the least. 

simi.sampler <- function(data, class, compare.with = 0, plot = F) {
  out.col <- which(colnames(data) == 'class')
  vars <- names(data[,-out.col])
  vars <- vars[vars != 'flag']
  vars <- vars[vars != 'tag']
  sampler.data <- data[vars]
  sampler.data <- data.frame(apply(sampler.data, 2, as.numeric))
  sampler.data <- data.frame(scale(sampler.data, T, T))
  var.nums <- 1:dim(sampler.data)[2]
  classes <- length(unique(data$class))
  
  # compute mean values vector for each of the groups - for similarity
  for (i in 1:classes) {
    assign(paste('class.', as.character(i), '.vector', sep = ''), 
           apply(sampler.data[data$class == i,][,vars], 2, mean))
    assign(paste('class.', as.character(i), '.mag', sep = ''), 
           sqrt(sum(apply(sampler.data[data$class == i,][,vars], 2, mean)^2)))
  }
  
  # compute similarity of each group with itself - same class
  new.col <- dim(sampler.data)[2] + 1
  for (r in 1:nrow(sampler.data)) {
    for (i in 1:classes) {
      if (data$class[r] == i) {
        vec <- get(ls()[which(grepl(paste('.',as.character(i),'.vector',sep = ''), ls()))])
        mag <- get(ls()[which(grepl(paste('.',as.character(i),'.mag',sep = ''), ls()))]) 
        sampler.data[r ,new.col] <- sum(vec*sampler.data[r, 1:(new.col - 1)])/((mag*sqrt(sum(sampler.data[r, 1:(new.col - 1)]^2))))
      }
    }
  }
  
  # compute similarity between classes
  simi.df <- data.frame(matrix(ncol = classes, nrow = nrow(data)))
  for (i in 1:nrow(simi.df)) {
    for (j in 1:classes) {
      vec <- get(ls()[which(grepl(paste('.',as.character(j),'.vector',sep = ''), ls()))])
      mag <- get(ls()[which(grepl(paste('.',as.character(j),'.mag',sep = ''), ls()))]) 
      simi.df[i ,j] <- sum(vec*sampler.data[i, 1:(new.col - 1)])/((mag*sqrt(sum(sampler.data[i, 1:(new.col - 1)]^2))))
    }
  }
  names(simi.df) <- as.character(seq(1, classes))
  
  names(sampler.data)[new.col] <- 'class.similarity'
  simi.table <- cbind(sampler.data[new.col], simi.df)
  simi.table <- dplyr::mutate(simi.table, data$class)
  simi.table <- dplyr::mutate(simi.table, row.names(data))
  simi.table <- dplyr::mutate(simi.table, data$flag)
  colnames(simi.table)[c(1, (2 + classes):dim(simi.table)[2])] <- c('same.class','class','Name', 'flag')
  
  
  plot.sim.before <- ggplot(simi.table, aes(simi.table[,compare.with + 1], y = class, label = Name)) +
    geom_point(aes(color = class)) +
    geom_text_repel(aes(label = Name), max.overlaps = 25) +
    theme(axis.line.x = element_line(size = 1, colour = "black"),
          axis.line.y = element_line(size = 1, colour = "black"),
          axis.text.x = element_text(colour = "black", size = 12,face = 'bold'),
          axis.text.y = element_text(colour = "black", size = 12,face = 'bold'),
          axis.title.x = element_text(colour = "black", size = 12,face = 'bold'),
          axis.title.y = element_text(colour = "black", size = 12,face = 'bold'),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), panel.background = element_blank(),
          legend.position = c(2,2)) +
    xlab(names(simi.table)[compare.with + 1]) + 
    ggtitle('Similarity Before Group Truncation')
  
  
  
  sample.size <-  min(summary(as.factor(data$class)))
  simi.class <- simi.table[simi.table$class == class, compare.with + 1]
  steps <-  sort(seq(min(simi.class), max(simi.class), (max(simi.class) - min(simi.class))/(sample.size - 1)))
  dis.mat <- matrix(ncol = length(steps), nrow = length(sort(simi.class)))
  for (i in 1:nrow(dis.mat)) {
    dis.mat[i, ] <- abs(simi.class[i] - steps)
  }
  pts <- vector()
  row.names(dis.mat) <- as.character(simi.table$flag[simi.table$class == class])
  if (length(steps) < length(simi.class)) {
    for (i in 1:ncol(dis.mat)) {
      drop <- which.min(dis.mat[, i])
      pts[i] <- simi.table[as.numeric(names(drop)), 1]
      dis.mat <- dis.mat[-drop, ]
    }
  } else {
    pts <- simi.table[as.numeric(row.names(dis.mat)), 1]
  }
  keep <- as.numeric(simi.table$flag[simi.table$same.class %in% pts])
  
  class.rows <- simi.table$flag[simi.table$class == class]
  if (min(class.rows) != 1) {
    truncated <- unique(c(1:(min(class.rows) + 1), keep, (max(class.rows) + 1):nrow(data)))
  } else {
    truncated <- unique(c(keep, (max(class.rows) + 1):nrow(data)))
  }
  simi.plot.data <- simi.table[truncated,][complete.cases(simi.table[truncated,]),]
  plot.sim.after <- ggplot(simi.plot.data,
                           aes(x = simi.plot.data[, compare.with + 1],
                               y = class, label = Name)) +
    geom_point(aes(color = class)) +
    geom_text_repel(aes(label = Name), max.overlaps = 25) +
    theme(axis.line.x = element_line(size = 1, colour = "black"),
          axis.line.y = element_line(size = 1, colour = "black"),
          axis.text.x = element_text(colour = "black", size = 12,face = 'bold'),
          axis.text.y = element_text(colour = "black", size = 12,face = 'bold'),
          axis.title.x = element_text(colour = "black", size = 12,face = 'bold'),
          axis.title.y = element_text(colour = "black", size = 12,face = 'bold'),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_blank(), panel.background = element_blank(),
          legend.position = c(2,2)) +
    xlab(names(simi.table)[compare.with + 1]) + 
    ggtitle('Similarity After Group Truncation')
  
  if (isTRUE(plot)) gridExtra::grid.arrange(plot.sim.before, plot.sim.after, ncol = 2)
  
  return(keep)
}
```

``` {r Model Search}
data50 <- data.table::fread('computed_descriptors_above50.csv', header = T) # Import data
data50 <- data.frame(data50)
row.names(data50) <- data50[,1]
data50 <- data50[,-1]
for(i in 1:ncol(data50)) data50[, i] <- as.factor(data50[, i])
data50 <- plyr::mutate(data50, flag = seq(1,nrow(data50)))

## Sampler data

sampl.dat <- data.table::fread('Monica_renum_04_2024_aldehyde_above50.csv', header = T) # Import data
sampl.dat <- data.frame(sampl.dat)
row.names(sampl.dat) <- sampl.dat[,1]
sampl.dat <- sampl.dat[,-1]
sampl.dat$class <- as.factor(sampl.dat$class)
sampl.dat[,c(1:60)] <- as.numeric(as.matrix(sampl.dat[,c(1:60)]))
sampl.dat <- plyr::mutate(sampl.dat, flag = seq(1,nrow(sampl.dat)))



one <- simi.sampler(sampl.dat, 1) # Sample from group 1, according to size of smallest group
two <- simi.sampler(sampl.dat, 2)
three <- simi.sampler(sampl.dat, 3)
four <- simi.sampler(sampl.dat, 4)

two_three <- simi.sampler(sampl.dat, 2, 3) # Sample from group 2 molecules that are similar to group 1

one_three <- simi.sampler(sampl.dat, 1, 3)

four_three <- simi.sampler(sampl.dat, 4, 3)

## Back to model search

# MP.models.gauss.union50 <- sub_model_log_McFadden(data = data50[c(three,
#                                                                   union(two, two_three),
#                                                                   union(one, one_three),
#                                                                   union(four, four_three)),],
#                                                 min = 4,
#                                                 max = 4)


MP.models.gauss.union50 <- data.frame(data.table::fread("fingerprints_model_4_features.csv"),
                                      check.names = F)[, -1]
test.data <- data50[c(three,
                                                                  union(two, two_three),
                                                                  union(one, one_three),
                                                                  union(four, four_three)),]

Test.set <- data50[-c(three,
                                                                  union(two, two_three),
                                                                  union(one, one_three),
                                                                  union(four, four_three)),]
```
## Published Model 

### 3rd Place
``` {r 1.1.1}

test.form <- "`class` ~ `fr_NH0` + `fr_aldehyde` + `fr_amide` + `fr_pyridine`"

knitr::kable(data.frame('formula' = test.form))

test <- nnet::multinom(test.form,
                       data = test.data,
                       maxit = 2000, 
                       trace = F)

mod.info(test, test.data, T, T)

ct.plot(class.table, 
        plot.title = 'Training Set',
        conformation = '3. 3rd place - 4 features')

prob.heatmap(test, test.data, 
        plot.title = 'Training Set',
        conformation = '3. 3rd place - 4 features')
```

```{r 1.1.2}

mod.info(test, Test.set, F)

ct.plot(class.table, 
        plot.title = 'Test Set',
        conformation = '3. 3rd place - 4 features')

prob.heatmap(test, Test.set, 
        plot.title = 'Test Set',
        conformation = '3. 3rd place - 4 features')

percentage <- round((sum(diag(table(predict(test, Test.set), Test.set$class))) / nrow(Test.set)) * 100, 1)
model.sum.1.1 <- data.frame('Training Accuracy' = Acc.print, 'Test Accuracy' = percentage)
row.names(model.sum.1.1) <- 'Full Model'
```

```{r 1.1.3}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_below50.csv'))
RN <- Predictions_data$NAMES
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for (i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

mod.info(test, Predictions_data, F)

ct.plot(class.table, 
        plot.title = 'External Validation',
        conformation = '3. 3rd Place - 4 features')

prob.heatmap(test, Predictions_data, 
        plot.title = 'External Validation',
        conformation = '3. 3rd Place - 4 features')
```

```{r 1.1.4}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_predictions.csv'))
RN <- Predictions_data$V1
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for(i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

pred.table <- data.frame(round(cbind(predict(test, Predictions_data),
                                     round(predict(test, Predictions_data, 'probs') * 100, 2)), 0))
names(pred.table)[1] <- c("Prediction")
knitr::kable(pred.table, caption = "External Validation", align = 'r')
```

## Model Analysis - Variable Impact

### 1. fr_NH0 Removed

``` {r 2.1.1}

test.form <- "`class` ~ `fr_aldehyde` + `fr_amide` + `fr_pyridine`"

knitr::kable(data.frame('formula' = test.form))

test <- nnet::multinom(test.form,
                       data = test.data,
                       maxit = 2000, 
                       trace = F)

mod.info(test, test.data, T, T)

ct.plot(class.table, 
        plot.title = 'Training Set',
        conformation = 'fr_NH0 Removed')

prob.heatmap(test, test.data, 
        plot.title = 'Training Set',
        conformation = 'fr_NH0 Removed')
```

```{r 2.1.2}

mod.info(test, Test.set, F)

ct.plot(class.table, 
        plot.title = 'Test Set',
        conformation = 'fr_NH0 Removed')

prob.heatmap(test, Test.set, 
        plot.title = 'Test Set',
        conformation = 'fr_NH0 Removed')

percentage <- round((sum(diag(table(predict(test, Test.set), Test.set$class))) / nrow(Test.set)) * 100, 1)
model.sum.1 <- data.frame('Training Accuracy' = Acc.print, 'Test Accuracy' = percentage)
row.names(model.sum.1) <- '1. fr_NH0 Removed'
```

```{r 2.1.3}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_below50.csv'))
RN <- Predictions_data$NAMES
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for (i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

mod.info(test, Predictions_data, F)

ct.plot(class.table, 
        plot.title = 'External Validation',
        conformation = 'fr_NH0 Removed')

prob.heatmap(test, Predictions_data, 
        plot.title = 'External Validation',
        conformation = 'fr_NH0 Removed')
```

### 2. fr_aldehyde Removed

``` {r 3.1.1}

test.form <- "`class` ~ `fr_NH0` + `fr_amide` + `fr_pyridine`"

knitr::kable(data.frame('formula' = test.form))

test <- nnet::multinom(test.form,
                       data = test.data,
                       maxit = 2000, 
                       trace = F)

mod.info(test, test.data, T, T)

ct.plot(class.table, 
        plot.title = 'Training Set',
        conformation = 'fr_aldehyde Removed')

prob.heatmap(test, test.data, 
        plot.title = 'Training Set',
        conformation = 'fr_aldehyde Removed')
```

```{r 3.1.2}

mod.info(test, Test.set, F)

ct.plot(class.table, 
        plot.title = 'Test Set',
        conformation = 'fr_aldehyde Removed')

prob.heatmap(test, Test.set, 
        plot.title = 'Test Set',
        conformation = 'fr_aldehyde Removed')

percentage <- round((sum(diag(table(predict(test, Test.set), Test.set$class))) / nrow(Test.set)) * 100, 1)
model.sum.2 <- data.frame('Training Accuracy' = Acc.print, 'Test Accuracy' = percentage)
row.names(model.sum.2) <- '2. fr_aldehyde Removed'
```

```{r 3.1.3}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_below50.csv'))
RN <- Predictions_data$NAMES
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for (i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

mod.info(test, Predictions_data, F)

ct.plot(class.table, 
        plot.title = 'External Validation',
        conformation = 'fr_aldehyde Removed')

prob.heatmap(test, Predictions_data, 
        plot.title = 'External Validation',
        conformation = 'fr_aldehyde Removed')
```

### 3. fr_amide Removed

``` {r 4.1.1}

test.form <- "`class` ~ `fr_NH0` + `fr_aldehyde` + `fr_pyridine`"

knitr::kable(data.frame('formula' = test.form))

test <- nnet::multinom(test.form,
                       data = test.data,
                       maxit = 2000, 
                       trace = F)

mod.info(test, test.data, T, T)

ct.plot(class.table, 
        plot.title = 'Training Set',
        conformation = 'fr_amide Removed')

prob.heatmap(test, test.data, 
        plot.title = 'Training Set',
        conformation = 'fr_amide Removed')
```

```{r 4.1.2}

mod.info(test, Test.set, F)

ct.plot(class.table, 
        plot.title = 'Test Set',
        conformation = 'fr_amide Removed')

prob.heatmap(test, Test.set, 
        plot.title = 'Test Set',
        conformation = 'fr_amide Removed')

percentage <- round((sum(diag(table(predict(test, Test.set), Test.set$class))) / nrow(Test.set)) * 100, 1)
model.sum.3 <- data.frame('Training Accuracy' = Acc.print, 'Test Accuracy' = percentage)
row.names(model.sum.3) <- '3. fr_amide Removed'
```

```{r 4.1.3}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_below50.csv'))
RN <- Predictions_data$NAMES
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for (i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

mod.info(test, Predictions_data, F)

ct.plot(class.table, 
        plot.title = 'External Validation',
        conformation = 'fr_amide Removed')

prob.heatmap(test, Predictions_data, 
        plot.title = 'External Validation',
        conformation = 'fr_amide Removed')
```

### 4. fr_pyridine Removed

``` {r 5.1.1}

test.form <- "`class` ~ `fr_NH0` + `fr_aldehyde` + `fr_amide`"

knitr::kable(data.frame('formula' = test.form))

test <- nnet::multinom(test.form,
                       data = test.data,
                       maxit = 2000, 
                       trace = F)

mod.info(test, test.data, T, T)

ct.plot(class.table, 
        plot.title = 'Training Set',
        conformation = 'fr_pyridine Removed')

prob.heatmap(test, test.data, 
        plot.title = 'Training Set',
        conformation = 'fr_pyridine Removed')
```

```{r 5.1.2}

mod.info(test, Test.set, F)

ct.plot(class.table, 
        plot.title = 'Test Set',
        conformation = 'fr_pyridine Removed')

prob.heatmap(test, Test.set, 
        plot.title = 'Test Set',
        conformation = 'fr_pyridine Removed')

percentage <- round((sum(diag(table(predict(test, Test.set), Test.set$class))) / nrow(Test.set)) * 100, 1)
model.sum.4 <- data.frame('Training Accuracy' = Acc.print, 'Test Accuracy' = percentage)
row.names(model.sum.4) <- '4. fr_pyridine Removed'
```

```{r 5.1.3}
Predictions_data <- data.frame(data.table::fread('computed_descriptors_below50.csv'))
RN <- Predictions_data$NAMES
Predictions_data <- Predictions_data[,-1]
row.names(Predictions_data) <- RN
for (i in 1:ncol(Predictions_data)) Predictions_data[, i] <- as.factor(Predictions_data[, i])

mod.info(test, Predictions_data, F)

ct.plot(class.table, 
        plot.title = 'External Validation',
        conformation = 'fr_pyridine Removed')

prob.heatmap(test, Predictions_data, 
        plot.title = 'External Validation',
        conformation = 'fr_pyridine Removed')
```

## Section Results Summary
```{r sum 1}
knitr::kable(rbind(model.sum.1.1, model.sum.1, model.sum.2, model.sum.3, model.sum.4))
```